# ===== Snake file for processing Bowtie ================================

# Configure shell for all rules
shell.executable("/bin/bash")
shell.prefix("source ~/.bash_profile; set -o nounset -o pipefail -o errexit -x; ")

# Python packages
import os
import sys
import yaml
from pathlib import Path

# Include custom Python functions
include: "funs.py"

# ------------------------------------------------------------------------------
# Load main genome config
# ------------------------------------------------------------------------------

GENOME = config["GENOME"]
GENOME_CONFIG = Path("pipelines/ref") / f"{GENOME}.yaml"

if not GENOME_CONFIG.exists():
    sys.exit(f"ERROR: {GENOME} is not a valid GENOME selection.")

# Load the main config file into Snakemake's config dictionary
configfile: str(GENOME_CONFIG)

# ------------------------------------------------------------------------------
# Load additional genome-specific configs manually
# ------------------------------------------------------------------------------

INDEX_SAMPLE = config['INDEX_SAMPLE']
INDEX_SPIKE = config['INDEX_SPIKE']

# Paths to additional config files
GENOME_CONFIG1 = Path("pipelines/ref") / f"{INDEX_SAMPLE}.yaml"
GENOME_CONFIG2 = Path("pipelines/ref") / f"{INDEX_SPIKE}.yaml"

# Validate existence
if not GENOME_CONFIG1.exists():
    sys.exit(f"ERROR: {INDEX_SAMPLE} is not a valid INDEX_SAMPLE selection.")
if not GENOME_CONFIG2.exists():
    sys.exit(f"ERROR: {INDEX_SPIKE} is not a valid INDEX_SPIKE selection.")

# Load additional configs
with open(GENOME_CONFIG1) as f:
    config1 = yaml.safe_load(f)

with open(GENOME_CONFIG2) as f:
    config2 = yaml.safe_load(f)

# ------------------------------------------------------------------------------
# Assign parameters from configs
# ------------------------------------------------------------------------------

# From main config
PROJ         = config.get("PROJ")
RAW_DATA     = config.get("RAW_DATA")
ALL_SAMPLES  = config.get("SAMPLES")
SEQ_DATE     = config.get("SEQ_DATE")
INDEX_PATH   = config.get("INDEX_PATH")
INDEX_MAP    = config.get("INDEX_MAP")
CMD_PARAMS   = config.get("CMD_PARAMS")
COLORS       = config.get("COLORS")
NORM         = config.get("NORM")
ORIENTATION  = config.get("ORIENTATION")
USER         = config.get("USER")

# From additional configs
FA_SAMPLE    = config1.get("FA_SAMPLE")
FA_SPIKE     = config2.get("FA_SAMPLE")
FC_FILE      = config1.get("GTF")
FC_SPIKE     = config2.get("GTF")
MASK         = config1.get("MASK")


# Directories for data and scripts
FASTQ_DIR = PROJ + "/fastqs"

os.makedirs(FASTQ_DIR, exist_ok = True)

# Simplify ALL_SAMPLES dictionary
# ALL_SAMPLES = {'SECTION-1':{'SAMPLING-GROUP-1':{newname1:[fastq1],newname2:[fastq2]}}} or {'SECTION-1':{'SAMPLING-GROUP-1':{newname1:[fastq1,input1],newname2:[fastq2,input2]}}}
# collapse sections and combine subsampling groups
def process_samples(all_samples):
    SAMPLES = {}  # SAMPLES {newname1:[fastq1],newname2:[fastq2]}
    SAMPIN = {}   # SAMPIN {newname1:[fastq1,input1],newname2:[fastq2,input2]}
    GROUPS = {}   # GROUPS {SAMPLING-GROUP-1:[fastq1], SAMPLING-GROUP-2:[fastq2]}

    for section, pairs in all_samples.items():
        for pair_name, samples in pairs.items():
            for sample_name, values in samples.items():
                fastq = values[0]
                input_file = values[1] if len(values) > 1 else None

                # Populate SAMPLES
                SAMPLES.setdefault(sample_name, []).append(fastq)

                # Populate SAMPIN
                if input_file:
                    SAMPIN[sample_name] = [fastq, input_file]
                else:
                    SAMPIN[sample_name] = [fastq]

                # Populate GROUPS
                GROUPS.setdefault(pair_name, []).append(fastq)

    return SAMPLES, SAMPIN, GROUPS

SAMPLES, SAMPIN, GROUPS = process_samples(ALL_SAMPLES)

# unpack samples and groups
SAMS = [[y, x] for y in SAMPIN for x in SAMPIN[y]]
NAMS = [x[0] for x in SAMS] # newnames
SAMS = [x[1] for x in SAMS] # samples
GRPS = [[y, x] for y in GROUPS for x in GROUPS[y]]
GRPS = [x[0] for x in GRPS] # groups
NAMS_UNIQ = list(dict.fromkeys(NAMS))
GRPS_UNIQ = list(dict.fromkeys(GRPS))
SAMS_UNIQ = list(dict.fromkeys(SAMS))

# Print summary of samples and groups
print("SAMPLES (%s): %s\n" % (len(SAMPLES), SAMPLES))
print("GROUPS (%s): %s\n" % (len(GROUPS), GROUPS))
print("SAMPIN (%s): %s\n" % (len(SAMPIN), SAMPIN))
print("SAMS (%s): %s\n" % (len(SAMS), SAMS))
print("NAMS (%s): %s\n" % (len(NAMS), NAMS))
print("GRPS (%s): %s\n" % (len(GRPS), GRPS))
print("SAMS_UNIQ (%s): %s\n" % (len(SAMS_UNIQ), SAMS_UNIQ))
print("NAMS_UNIQ (%s): %s\n" % (len(NAMS_UNIQ), NAMS_UNIQ))
print("GRPS_UNIQ (%s): %s\n" % (len(GRPS_UNIQ), GRPS_UNIQ))

# Wildcard constraints
WILDCARD_REGEX = "[a-zA-Z0-9_\-]+" # Matches alphanumeric characters, underscores, and hyphens

wildcard_constraints:
    sample = WILDCARD_REGEX,
    newnam = WILDCARD_REGEX,
    group  = WILDCARD_REGEX   

# Create symlinks for fastqs
FASTQS = [_get_fqs(x, RAW_DATA, FASTQ_DIR) for x in SAMS_UNIQ]
FASTQS = sum(FASTQS, [])

COLS_DICT = _get_colors(NAMS_UNIQ, COLORS)

NORMS = _get_normtype(CMD_PARAMS["bamCoverage"],NORM,CMD_PARAMS["bamCoverageBL"],ORIENTATION)

BAM_PATH = _get_bampath(NORM)

# Final output files
rule all:
    input:
        # process summary files
        expand("{proj}/stats/{proj}_{step}.tsv", proj=PROJ, step=["fastqc", "cutadapt", "aligned"]),
        
        # bam URL
        #PROJ + "/URLS/" + PROJ + "_" + INDEX_SAMPLE + "_bam_URL.txt",
        #PROJ + "/URLS/" + PROJ + "_" + INDEX_SPIKE + "_bam_URL.txt",
        
        # subsample bams URL's
        #PROJ + "/URLS/" + PROJ + "_" + INDEX_SAMPLE + "_subsample_bam_URL.txt",
        #PROJ + "/URLS/" + PROJ + "_" + INDEX_SPIKE + "_subsample_bam_URL.txt",
        
        # results
        PROJ + "/report/" + PROJ + "_results.tsv",
        
        # bamCoverage
        PROJ + "/URLS/" + PROJ + "_" + INDEX_SAMPLE + NORMS + "_bw_URL.txt",
        PROJ + "/URLS/" + PROJ + "_" + INDEX_SPIKE + "_norm_CPM_bw_URL.txt"


# Run FastQC
include: "rules/01_fastqc.snake"
# Run cutadapt
include: "rules/01_UMI_cutadapt.snake"
# Align reads 
include: "rules/02a_align_bowtie_SE.snake"
include: "rules/02b_align_UMI_samtools.snake"
include: "rules/02c_align_URLS.snake"
include: "rules/02s_align_mask_subsample.snake"
# Results
include: "rules/03a_featureCounts.snake"
include: "rules/03b_results_spikeIN_UMI.snake"
include: "rules/03b_PCAplot.snake"
# BW with deeptools bamCoverage
include: "rules/04_bamCoverage_stranded.snake"

