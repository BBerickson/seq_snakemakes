# ====== Rules for removing PCR duplicates with UMI-tools ======================


# Remove duplicate reads
rule dedup_spikein:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai"
    output:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup.bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup.bam.bai",
        bamt  = temp(PROJ + "/bams/{sample}_temp.bam"),
        bait  = temp(PROJ + "/bams/{sample}_temp2.bam"),
        counts = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup.txt",
        stats  = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup_stats.txt"
    params:
        job_name = "{sample}_dedup_spikein",
        memory   = MEMORY * 3,
        args     = CMD_PARAMS["umi_tools"],
        samp     = "{sample}_" + INDEX_SPIKE,
        mask     = MASK,
        log      = PROJ + "/logs/{sample}_dedup.out"
    log:
        out = PROJ + "/logs/{sample}_dedup_spikein.out",
        err = PROJ + "/logs/{sample}_dedup_spikein.err"
    message:
        "Removing duplicates for {wildcards.sample}"
    threads:
        1
    shell:
        """
        umi_tools dedup \
            {params.args} \
            -I {input.bam} \
            -S {output.bamt} \
            -L {output.stats}
        
        samtools view -L {params.mask} -b {output.bamt} -U {output.bam} > {output.bait}

        samtools index {output.bam}
        
        echo "{params.samp} Filtered_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" > {output.counts}
        """

# Create duplication summary
rule dedup_summary_spikein:
    input:
        sorted(expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup_stats.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_dedup.tsv"
    params:
        job_name = PROJ + "_dedup_summary_spikein",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_dedup_summary_spikein.out",
        err = PROJ + "/logs/" + PROJ + "_dedup_summary_spikein.err"
    message:
        "Creating " + PROJ + " dedup summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            metrics = [
                "Input Reads: [0-9]+",
                "Number of reads out: [0-9]+",
                "Total number of positions deduplicated: [0-9]+",
                "Mean number of unique UMIs per position: [0-9\.]+",
                "Max. number of unique UMIs per position: [0-9]+"
            ]

            for file in input:
                name  = os.path.basename(file)
                name  = re.sub("_dedup_stats.txt", "", name)

                for line in open(file, "r"):
                    for metric in metrics:
                        met = re.search(metric, line)

                        if met:
                            met = met.group(0)
                            num = re.search("[0-9\.]+$", met).group(0)
                            met = re.sub(": [0-9\.]+$", "", met)

                            out.write("%s\t%s\t%s\n" % (name, met, num))


