# ====== Rules for subsampleing ======================


# Subsample libraries to equalize read counts for downstream analysis of each gene
rule subsample_gene:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
        bam   = temp(PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_dedup_filtered_gene.bam"),
        bam2  = temp(PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_dedup_filtered_temp.bam"),
        bam3  = temp(PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_dedup_filtered_temp2.bam"),
        bai   = temp(PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_dedup_filtered_gene.bam.bai"),
        stats = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_gene_filtered.txt"
    params:
        job_name = "filtered_gene{sample}",
        genes    = GENES,
        mask     = MASK,
        memory   = MEMORY,
        sortname = "{sample}"
    log:
        out = PROJ + "/logs/filtered_gene_{sample}_" + INDEX_SAMPLE + ".out",
        err = PROJ + "/logs/filtered_gene_{sample}_" + INDEX_SAMPLE + ".err"
    message:
        "Subsampling reads for {wildcards.sample} INDEX_SAMPLE "
    threads:
        16
    shell:
        """
          # filter reads to focus on gene regions
          samtools view -@ {threads} \
            -L {params.genes} -b {input.bam} \
            | samtools view -L {params.mask} -b - -U {output.bam2} > {output.bam3}
            
            samtools sort {output.bam2} -T {params.sortname} -@ {threads} -O bam \
            > {output.bam}
          
          samtools index -@ {threads} {output.bam}
          
        echo "{params.sortname} Aligned reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" >> {output.stats}
        
        """


# Create bed files for RNA 3' end
rule create_beds:
    input:
        PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_dedup_filtered_gene.bam"
    output:
        bed   = temp(PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".3bed.gz"),
        stats = PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_3bed.txt"
    params:
        job_name = "{sample}_create_beds",
        memory   = MEMORY * 3,
        genes    = GENES,
        mask     = MASK
    log:
        out = PROJ + "/logs/{sample}_beds.out",
        err = PROJ + "/logs/{sample}_beds.err"
    benchmark:
        PROJ + "/benchmarks/{sample}_beds.tsv"
    message:
        "Creating bed files for {wildcards.sample}"
    threads:
        12
    shell:
       """
       # Create bed file for aligned reads
       # read coordinates to the RNA 3' end.
       bamToBed -i {input} \
           | awk -v OFS="\t" -v sam="{wildcards.sample}" '{{
               if ($6 == "+") {{
                   $2 = $3 - 1;
               }} else {{
                   $3 = $2 + 1;
               }};
               count += 1;
               print $0, $3 - $2
           }} END {{
               print sam, "Filtered reads", count \
                   >> "{output.stats}";
           }}' \
           | sort -S1G --parallel={threads} -k1,1 -k2,2n \
           | pigz -p {threads} \
           > {output.bed}
       """

# Create filtering summary
rule filt_summary:
    input:
        expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_gene_filtered.txt",
            sample = SAMS_UNIQ
        ),
        expand(
            PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_3bed.txt",
            sample = SAMS_UNIQ
        )
    output:
        PROJ + "/stats/"+ PROJ + "_filt.tsv"
    params:
        job_name = PROJ + "_filt_summary",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_filt_summary.out",
        err = PROJ + "/logs/" + PROJ + "_filt_summary.err"
    message:
        "Creating " + PROJ + " read filtering summary"
    threads:
        1
    shell:
        """
        file_arr=({input})

        for file in ${{file_arr[@]}}
        do
            cat $file \
                >> {output}
        done
        """


# Intersect reads with regions for gene subsampling
rule subsample_metaplot_beds:
    input:
        PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".3bed.gz"
    output:
        S    = temp(PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{sub_region}_S.bed.gz"),
        AS   = temp(PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{sub_region}_AS.bed.gz"),
        S_N  = temp(PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{sub_region}_S_N.bed.gz"),
        AS_N = temp(PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{sub_region}_AS_N.bed.gz")
    params:
        job_name = "{sample}_{sub_region}_metaplot_beds",
        memory   = MEMORY * 2,
        bed      = lambda wildcards: GENE_SUB_BEDS[wildcards.sub_region]
    log:
        out = PROJ + "/logs/{sample}_{sub_region}_metaplot_beds.out",
        err = PROJ + "/logs/{sample}_{sub_region}_metaplot_beds.err"
    message:
        "Intersecting reads for {wildcards.sample} {wildcards.sub_region}"
    threads:
        6
    shell:
        """
        source {SRC}/funs.sh

        intersect_reads \
            {input} \
            {params.bed} \
            {threads} \
            {output.S} \
            {output.S_N} \
            "-wa -c -s"
            
        intersect_reads \
            {input} \
            {params.bed} \
            {threads} \
            {output.AS} \
            {output.AS_N} \
            "-wa -c -S"

        """

# Identify lowest counts for the gene among all samples in the sampling group
# for input, need to pull all files for the sampling group, but need each job
# to only include files for one sampling region
rule gene_subsample_1:
    input:
        lambda wildcards: expand(
            PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{{sub_region}}_S.bed.gz",
            sample = SAMPLES[wildcards.group]
        )
    output:
        temp(PROJ + "/stats/{group}_{sub_region}_summary.tsv")
    params:
        job_name = "{group}_{sub_region}_summary",
        memory   = MEMORY * 2
    log:
        out = PROJ + "/logs/{group}_{sub_region}_summary.out",
        err = PROJ + "/logs/{group}_{sub_region}_summary.err"
    benchmark:
        PROJ + "/benchmarks/{group}_{sub_region}_genes_summary.tsv"
    message:
        "Subsampling reads for {wildcards.group} {wildcards.sub_region}"
    threads:
        1
    run:
        # Persistent dictionary for storing minimum gene counts for subsampling
        # clear the dictionary to remove results from previous runs
        GENES_DICT = PersistentDict("GENES_DICT_" + wildcards.group + "_" + wildcards.sub_region)
        GENES_DICT.clear()

        # Get list of genes present in the first input file
        # could use any file in the group
        with gzip.open(input[0], "rt") as f:
            genes = [l.strip().split("\t")[3] for l in f]

        # Create set with counts for each gene and sample file
        for gene in genes:
            gene_set = set()

            for file in input:
                f = gzip.open(file, "rt")

                found = False

                for l in f:
                    l = l.strip().split("\t")
                    g = l[3]
                    c = int(l[6])

                    if g == gene:
                        gene_set.add(c)

                        found = True

                        break

                if not found:
                    gene_set.add(0)  # report 0 counts if gene not found

            # Store minimum count for the group in persistent dict
            min_reads = min(gene_set)

            GENES_DICT.store(gene, min_reads)

            # Save minimum counts in summary file
            with open(output[0], "a") as out:
                out.write("%s\t%s\tSampled reads\t%s\n" % (wildcards.group, gene, min_reads))


# Subsample reads so each gene for each sample has the same total counts for
# the subsampling region
rule gene_subsample_2:
    input:
        reads = PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".3bed.gz",
        bed   = PROJ + "/beds/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + "_{sub_region}_S.bed.gz",
        sum   = PROJ + "/stats/{group}_{sub_region}_summary.tsv"
    output:
        reads = temp(PROJ + "/beds/{sample}_{group}_{sub_region}_reads.bed.gz")
    params:
        job_name = "{sample}_{group}_{sub_region}_2",
        memory   = MEMORY * 2
    log:
        out = PROJ + "/logs/{sample}_{group}_{sub_region}_2.out",
        err = PROJ + "/logs/{sample}_{group}_{sub_region}_2.err"
    benchmark:
        PROJ + "/benchmarks/{sample}_{group}_{sub_region}_2.tsv"
    message:
        "Subsampling reads for {wildcards.sample} {wildcards.group} {wildcards.sub_region}"
    threads:
        6
    shell:
        """
        # Intersect with subsampling region
        # this produces a bed file that only containing reads to use for
        # subsampling
        zcat {input.reads} \
            | bedtools intersect -sorted -s -a {input.bed} -b - \
            | sort -S1G --parallel={threads} -k1,1 -k4,4 -k2,2n \
            | pigz -p {threads} \
            > {output.reads}
        """
        
        
# Subsample reads so each gene for each sample has the same total counts for
# the subsampling region
rule gene_subsample_3:
    input:
        reads = PROJ + "/beds/{sample}_{group}_{sub_region}_reads.bed.gz"
    output:
        tmp   = temp(PROJ + "/beds/{sample}_{group}_{sub_region}_tmp.bed")
    params:
        job_name = "{sample}_{group}_{sub_region}_3",
        memory   = MEMORY * 2
    log:
        out = PROJ + "/logs/{sample}_{group}_{sub_region}_3.out",
        err = PROJ + "/logs/{sample}_{group}_{sub_region}_3.err"
    benchmark:
        PROJ + "/benchmarks/{sample}_{group}_{sub_region}_3.tsv"
    message:
        "Subsampling reads for {wildcards.sample} {wildcards.group} {wildcards.sub_region}"
    threads:
        6
    run:
        # Persistent dictionary
        # this contains the minimum number of reads aligning to each gene for
        # the subsampling group
        GENES_DICT = PersistentDict("GENES_DICT_" + wildcards.group + "_" + wildcards.sub_region)
 
        # Create dictionary containing reads to use for subsampling
        reads = gzip.open(input.reads, "rt")

        READS_DICT = dict()
 
        for l in reads:
            l    = l.strip()
            gene = l.split("\t")[3]

            if gene not in READS_DICT:
                READS_DICT[gene] = [l]
 
            else:
                READS_DICT[gene].append(l)

        # Subsample reads in READS_DICT and write bed file
        # pull minimum read count for the gene from GENES_DICT
        with open(output.tmp, "a") as out:
            for gene in READS_DICT.keys():

                MIN_READS = GENES_DICT.fetch(gene)

                if len(READS_DICT[gene]) > MIN_READS:
                    READS_DICT[gene] = random.sample(READS_DICT[gene], MIN_READS)
 
                for l in READS_DICT[gene]:
                    out.write("%s\n" % l)


# Subsample reads so each gene for each sample has the same total counts for
# the subsampling region
rule gene_subsample_4:
    input:
        tmp   = PROJ + "/beds/{sample}_{group}_{sub_region}_tmp.bed"
    output:
        sub   = temp(PROJ + "/beds/{sample}_{group}_{sub_region}.bed.gz")
    params:
        job_name = "{sample}_{group}_{sub_region}",
        memory   = MEMORY * 2
    log:
        out = PROJ + "/logs/{sample}_{group}_{sub_region}_4.out",
        err = PROJ + "/logs/{sample}_{group}_{sub_region}_4.err"
    benchmark:
        PROJ + "/benchmarks/{sample}_{group}_{sub_region}_4.tsv"
    message:
        "Subsampling reads for {wildcards.sample} {wildcards.group} {wildcards.sub_region}"
    threads:
        6
    shell:
        """
        cat {input.tmp} \
            | sort -S1G --parallel={threads} -k1,1 -k2,2n \
            | pigz -p {threads} \
            > {output.sub}
        """



                   