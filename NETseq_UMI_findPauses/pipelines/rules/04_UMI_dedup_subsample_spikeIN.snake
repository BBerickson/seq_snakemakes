# ====== Rules for subsampleing ======================

# gets number for norm fraction
def _get_norm(wildcards):
    sample = wildcards.sample + "_" + INDEX_SAMPLE
    filename =  PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv"
    num = 0
    with open(filename, "r") as file:
      for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 3 and parts[0] == sample:
          num = parts[2]
          break
    return float(num)
    
        

# calculating subsample numbers for group
rule calculating_subsample_spikein:
    input:
        lambda wildcards: expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup.txt",
            sample = SAMPLES[wildcards.group]
        )
    output:
        temp(PROJ + "/bams/{group}_" + INDEX_SPIKE + "_subsample_frac.txt")
    params:
        job_name = "{group}_subsample_spikein",
        group   = "{group}",
        memory   = 4
    log:
        out = PROJ + "/logs/{group}_subsample_spikein.out",
        err = PROJ + "/logs/{group}_subsample_spikein.err"
    message:
        "getting subsample norm for {wildcards.group}"
    threads: 
        1
    script:
        '../R_scripts/count_subsample.R'


# Combine bamCoverage summaries
rule subsample_summary_frac_spikein:
    input:
        sorted(expand(
            PROJ + "/bams/{group}_" + INDEX_SPIKE + "_subsample_frac.txt",
            group = GRPS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv"
    params:
        job_name = PROJ + "_subsample_summary_spikein",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_subsample_summary_spikein.out",
        err = PROJ + "/logs/" + PROJ + "_subsample_summary_spikein.err"
    message:
        "Creating " + PROJ + " subsample summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)


# Subsample libraries to equalize read counts for downstream analysis
rule subsample_whole_spikein:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup.bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup.bam.bai",
        counts = expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup.txt",
            sample = SAMS_UNIQ
        ),
        counts2 = PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv",
        stats = PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_dedup.tsv"
    output:
        bam   = temp(PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup_subsample.bam"),
        bai   = temp(PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup_subsample.bam.bai"),
        stats = temp(PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup_subsample.txt")
    params:
        job_name = "subsample_whole{sample}_spikein",
        memory   = MEMORY,
        scale    = _get_norm,
        mysample = "{sample}_" + INDEX_SPIKE
    log:
        out = PROJ + "/logs/subsample_spike_{sample}_" + INDEX_SPIKE + ".out",
        err = PROJ + "/logs/subsample_spike_{sample}_" + INDEX_SPIKE + ".err"
    message:
        "Subsampling reads for {wildcards.sample} INDEX_SPIKE "
    threads:
        16
    shell:
        """
          # find the min read count, calcuate the fraction of the reads to keep, samtools subset 
          samtools view -@ {threads} -s {params.scale} -b {input.bam} > {output.bam}
          
          samtools index -@ {threads} {output.bam}
          
        echo "{params.mysample} Sampled_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" >> {output.stats}
        
        """


# Create subsample_whole summary
rule subsample_summary_spikein:
    input:
        expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup.txt",
            sample =SAMS_UNIQ),
        expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_dedup_subsample.txt",
            sample =SAMS_UNIQ)
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample.tsv"
    params:
        job_name = PROJ + "_subsample_whole_summary_spikein",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_subsample_whole_summary_spikein.out",
        err = PROJ + "/logs/" + PROJ + "_subsample_whole_summary_spikein.err"
    message:
        "Creating " + PROJ + " subsample_whole summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)


# make URLS and copy files to sandbox
rule my_subsample_whole_url_spikein:
    input:
        bam   = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup_subsample.bam",
        bai   = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_dedup_subsample.bam.bai"
    output:
        temp(PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_subsample_url.txt")
    params:
        job_name = PROJ + "_subsample_whole_url_spikein",
        memory   = 4,
        url      = "http://amc-sandbox.ucdenver.edu/" + USER + "/" + PROJ + "/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam"
    log:
        out = PROJ + "/logs/{sample}_" + PROJ + "_subsample_whole_url_spikein.out",
        err = PROJ + "/logs/{sample}_" + PROJ + "_subsample_whole_url_spikein.err"
    message:
        "Creating " + PROJ + " subsample_whole url"
    threads:
        1
    shell:
        """
          echo "{params.url}" >> {output}
          ssh amc-sandbox 'mkdir -p ./public_html/{PROJ}'
          scp {input.bam}* amc-sandbox:./public_html/{PROJ}
          
        """
                    
# Combine URLS
rule my_subsample_whole_url_summary_spikein:
    input:
        sorted(expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_subsample_url.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/URLS/" + PROJ + "_" + INDEX_SPIKE + "_dedup_subsample_whole_bam_URL.txt"
    params:
        job_name = PROJ + "_dedup_subsample_whole_bam_URL_spikein",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_dedup_subsample_whole_bam_URL_spikein.out",
        err = PROJ + "/logs/" + PROJ + "_dedup_subsample_whole_bam_URL_spikein.err"
    message:
        "Creating " + PROJ + " _dedup_subsample_whole_bam_URL"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)
                    
                    