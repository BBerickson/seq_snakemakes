# ===== Snake file for processing Bowtie ================================

from pytools.persistent_dict import PersistentDict
import os
import glob
import re
import subprocess
import gzip
import random

# Configure shell for all rules
shell.executable("/bin/bash")
shell.prefix("source ~/.bash_profile; set -o nounset -o pipefail -o errexit -x; ")

#include my python common functions
include: "commonFun.snake"

# Parameters from config.yaml
PROJ            = config["PROJ"]
RAW_DATA        = config["RAW_DATA"]
ALL_SAMPLES     = config["SAMPLES"]
SEQ_DATE        = config["SEQ_DATE"]
BARCODES        = config["BARCODES"]
INDEX_PATH      = config["INDEX_PATH"]
INDEX_MAP       = config["INDEX_MAP"]
INDEX_SAMPLE    = config["INDEX_SAMPLE"]
INDEX_SPIKE     = config["INDEX_SPIKE"]
FA_SAMPLE       = config["FA_SAMPLE"]
FA_SPIKE        = config["FA_SPIKE"]
MASK            = config["MASK"]
SNRNA           = config["SNRNA"]
SPIKE_SNRNA     = config["SPIKE_SNRNA"]
ENRICHED_SAMPLE = config["ENRICHED_SAMPLE"]
ENRICHED_SPIKE  = config["ENRICHED_SPIKE"]
CMD_PARAMS      = config["CMD_PARAMS"]
SFX             = config["SFX"]
COLORS          = config["COLORS"]
NORM            = config["NORM"]
USER            = config["USER"]
MEMORY          = config["MEMORY"]

# Directories for data and scripts
FASTQ_DIR = PROJ + "/fastqs"

# Create symlinks for fastqs
if not os.path.exists(FASTQ_DIR):
    os.makedirs(FASTQ_DIR)

# Simplify ALL_SAMPLES dictionary
# subsampling groups can be listed in multiple sections
# collapse sections and combine subsampling groups
SAMPLES = {} # SAMPLES {newname1:[fastq1],newname2:[fastq2]}
GROUPS = {} # GROUPS {group:[fastq1,fastq2]} 
for _, d in ALL_SAMPLES.items():
    for key, value in d.items():

        if not isinstance(value, dict):
            value = {key: value}  # make it a dictionary for uniformity

        for sub_keys, sub_value in value.items():
            if key in GROUPS:
                if sub_value not in GROUPS[key]:
                    GROUPS[key].append(sub_value)
            else:
                GROUPS[key] = [sub_value]
                
            if sub_keys in SAMPLES:
                if sub_value not in SAMPLES[sub_keys]:
                    SAMPLES[sub_keys].append(sub_value)

            else:
                SAMPLES[sub_keys] = [sub_value]

# unpack samples and groups
SAMS = [[y, x] for y in SAMPLES for x in SAMPLES[y]]
NAMS = [x[0] for x in SAMS]
GRPS = [[y, x] for y in GROUPS for x in GROUPS[y]]
SAMS = [x[1] for x in GRPS]
GRPS = [x[0] for x in GRPS]
NAMS_UNIQ = list(dict.fromkeys(NAMS))
GRPS_UNIQ = list(dict.fromkeys(GRPS))
SAMS_UNIQ = list(dict.fromkeys(SAMS))

# Print summary of samples and groups
print("SAMPLES (%s): %s\n" % (len(SAMPLES), SAMPLES))
print("GROUPS (%s): %s\n" % (len(GROUPS), GROUPS))
print("SAMS (%s): %s\n" % (len(SAMS), SAMS))
print("NAMS (%s): %s\n" % (len(NAMS), NAMS))
print("GRPS (%s): %s\n" % (len(GRPS), GRPS))
print("SAMS_UNIQ (%s): %s\n" % (len(SAMS_UNIQ), SAMS_UNIQ))
print("NAMS_UNIQ (%s): %s\n" % (len(NAMS_UNIQ), NAMS_UNIQ))
print("GRPS_UNIQ (%s): %s\n" % (len(GRPS_UNIQ), GRPS_UNIQ))

# Wildcard constraints
GRP_REGEX = "[a-zA-Z0-9_\-]+"

wildcard_constraints:
    sample  = "[a-zA-Z0-9_\-]+",
    group   = GRP_REGEX,
    sam_grp = "[a-zA-Z0-9_\-]+-[a-zA-Z0-9_\-]+"

FQ_SFX = _get_sfx(SFX)

FASTQS = [_get_fqs(x, y) for x in SAMS_UNIQ for y in FQ_SFX]

COLS_DICT = _get_colors(NAMS_UNIQ, COLORS)

NORMS = _get_normtype(CMD_PARAMS["bamCoverage"],NORM,CMD_PARAMS["bamCoverageBL"])

BAM_PATH = _get_bampath(NORM)

# Final output files
rule all:
    input:
        # FastQC
        PROJ + "/stats/" + PROJ + "_fastqc.tsv",
        
        # clumpify
        PROJ + "/stats/" + PROJ + "_clumpify.tsv",
        
        # bbduk
        PROJ + "/stats/" + PROJ + "_bbduk.tsv",

        # Align reads
        PROJ + "/stats/" + PROJ + "_aligned.tsv",
        
        # bam URL
        #PROJ + "/URLS/" + PROJ + "_" + INDEX_SAMPLE + "_bam_URL.txt",
        
        # results
        PROJ + "/stats/" + PROJ + "_results.tsv",
        PROJ + "/stats/multiBamSummary.png",
        #PROJ + "/stats/" + PROJ + "_fragment_results.tsv",
        
        # report
        PROJ + "/" + PROJ + "_" + INDEX_SAMPLE + "_qc_analysis.html",
        
        # bamCoverage
        PROJ + "/URLS/" + PROJ + "_" + INDEX_SAMPLE + "_" + NORMS + "_bw_URL.txt"


# Run FastQC
include: "rules/01_fastqc.snake"
# Run clumpify
include: "rules/01a_clumpify.snake"
# Run bbmerge
include: "rules/01b_bbduk.snake"
# Align reads 
include: "rules/02a_align_hisat.snake"
include: "rules/02b_align.snake"
include: "rules/02c_align_URLS.snake"
# Results
include: "rules/03_results.snake"
include: "rules/03_PCAplot.snake"
# BW with deeptools bamCoverage
include: "rules/04_bamCoverage_stranded.snake"

