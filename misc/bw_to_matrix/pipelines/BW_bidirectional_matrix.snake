# ===== Snake file for processing mNET-seq data ================================

# Configure shell for all rules
shell.executable("/bin/bash")
shell.prefix("[ -f ~/.bash_profile ] && source ~/.bash_profile; set -o nounset -o pipefail -o errexit -x; ")

# Python packages
import os
import sys
import yaml
from pathlib import Path

# Include custom Python functions
include: "funs.py"

# ------------------------------------------------------------------------------
# Load main genome config
# ------------------------------------------------------------------------------

GENOME = config["GENOME"]
GENOME_CONFIG = Path("pipelines/ref") / f"{GENOME}.yaml"

if not GENOME_CONFIG.exists():
    sys.exit(f"ERROR: {GENOME} is not a valid GENOME selection.")

# Load the main config file into Snakemake's config dictionary
configfile: str(GENOME_CONFIG)

# ------------------------------------------------------------------------------
# Load additional genome-specific configs manually
# ------------------------------------------------------------------------------

raw_indexes = config['INDEXES']
INDEXES = [raw_indexes] if isinstance(raw_indexes, str) else [raw_indexes[0]]


# Paths to additional config files
GENOME_CONFIG1 = Path("pipelines/ref") / f"{INDEXES[0]}.yaml"

# Validate existence
if not GENOME_CONFIG1.exists():
    sys.exit(f"ERROR: Config file not found for index '{INDEXES[0]}'. Expected at: {GENOME_CONFIG1}")

# Load additional configs
with open(GENOME_CONFIG1) as f:
    config1 = yaml.safe_load(f)

# ------------------------------------------------------------------------------
# Assign parameters from configs
# ------------------------------------------------------------------------------

# Docker container
singularity:
    config["CONTAINER"] 
    
# From main config
PROJ         = config.get("PROJ")
RAW_DATA     = config.get("RAW_DATA")
ALL_SAMPLES  = config.get("SAMPLES")
SEQ_DATE     = config.get("SEQ_DATE")
INDEX_PATH   = config.get("INDEX_PATH")
NORM         = config.get("NORM")
CMD_PARAMS   = config.get("CMD_PARAMS")
COLORS       = config.get("COLORS")
ORIENTATION  = config.get("ORIENTATION")
REGIONS      = config.get("REGIONS")
USER         = config.get("USER")

# From additional configs
FW_REF      = config1.get("FW_REF")
REV_REF     = config1.get("REV_REF")
GENELIST = config1.get("GENELIST") or ""


BW_DIR = PROJ + "/raw_bw"
os.makedirs(BW_DIR, exist_ok = True)

# Simplify ALL_SAMPLES dictionary
SAMPLES, SAMPIN, GROUPS, NORMMAP, PAIREDMAP = process_samples(
    ALL_SAMPLES, INDEXES, NORM, ORIENTATION
)

# make file suffix from bamCoverage settings and NORM 
for sample, norm_list in NORMMAP.items():
    updated_list = [
        (index, norm_value, _get_normtype(
            CMD_PARAMS["bamCoverage"],
            norm_value,
            CMD_PARAMS.get("bamCoverageBL", ""),
            ORIENTATION
        ))
        for index, norm_value in norm_list
    ]
    NORMMAP[sample] = updated_list
    
# Combine into a list of records with expanded NormMap
SAM_NORM = []
for key in SAMPIN:
    sam_value = SAMPIN[key][0]
    for index, norm, suffix in NORMMAP[key]:
        SAM_NORM.append([sam_value, key, index, norm, suffix])

# Create DataFrame
DF_SAM_NORM = pd.DataFrame(SAM_NORM, columns=['Sample', 'Newnam', 'Index', 'Norm', 'Suffix'])


# unpack samples and groups
SAMS = [[y, x] for y in SAMPLES for x in SAMPLES[y]]
NAMS = [x[0] for x in SAMS] # newnames
SAMS = [x[1] for x in SAMS] # samples
GRPS = [[y, x] for y in GROUPS for x in GROUPS[y]]
GRPS = [x[0] for x in GRPS] # groups
NAMS_UNIQ = list(dict.fromkeys(SAMS))
GRPS_UNIQ = list(dict.fromkeys(GRPS))
SAMS_UNIQ = list(dict.fromkeys(SAMS))

# Print summary of samples and groups
print("SAMPLES (%s): %s\n" % (len(SAMPLES), SAMPLES))
print("GROUPS (%s): %s\n" % (len(GROUPS), GROUPS))
print("SAMPIN (%s): %s\n" % (len(SAMPIN), SAMPIN))
print("SAMS_UNIQ (%s): %s\n" % (len(SAMS_UNIQ), SAMS_UNIQ))
print("NAMS_UNIQ (%s): %s\n" % (len(NAMS_UNIQ), NAMS_UNIQ))
print("GRPS_UNIQ (%s): %s\n" % (len(GRPS_UNIQ), GRPS_UNIQ))
print("NORMMAP (%s): %s\n" % (len(NORMMAP), NORMMAP))
print("PAIREDMAP (%s): %s\n" % (len(PAIREDMAP), PAIREDMAP))
print(DF_SAM_NORM.to_string(index=False))

# Wildcard constraints
WILDCARD_REGEX = "[a-zA-Z0-9_\-]+" # Matches alphanumeric characters, underscores, and hyphens

wildcard_constraints:
    sample = WILDCARD_REGEX,
    newnam = WILDCARD_REGEX,
    group  = WILDCARD_REGEX,
    index  = WILDCARD_REGEX,
    suffix = WILDCARD_REGEX,
    covarg = "[a-zA-Z0-9_.\\-]+",
    region = "543|5|3|PI|EI"

COLS_DICT = _get_colors(SAMS_UNIQ, COLORS)

NORMS = _get_normtype(CMD_PARAMS["bamCoverage"],NORM,CMD_PARAMS.get("bamCoverageBL", ""),ORIENTATION)

BAM_PATH = _get_bampath(NORM)

COVARGS = _get_all_matrixtypes(REGIONS,NORMS,CMD_PARAMS,GENELIST)

# Create the Cartesian product
product = [(s, i, v) for s in SAMS_UNIQ for i, v in zip(REGIONS, COVARGS)]

# Convert to DataFrame
REGIONS_COVARGS = pd.DataFrame(product, columns=['Newnam', 'Region', 'Value'])

print(REGIONS_COVARGS)

# Final output files
rule all:
    input:
        # bamCoverage
        expand(
            PROJ + "/bw/{newnam}_aligned_{index}_" + SEQ_DATE + "_norm_{suffix}_fw.bw",
            zip,
            newnam=DF_SAM_NORM['Newnam'],
            index=DF_SAM_NORM['Index'],
            suffix=DF_SAM_NORM['Suffix']
        ),
        expand(
            PROJ + "/bw/{newnam}_aligned_{index}_" + SEQ_DATE + "_norm_suffix}_rev.bw",
            zip,
            newnam=DF_SAM_NORM['Newnam'],
            index=DF_SAM_NORM['Index'],
            suffix=DF_SAM_NORM['Suffix']
        ),
        # matrix file
        expand(
          PROJ + "/URLS/{region}_{index}_" + SEQ_DATE + "_{covarg}_norm_{suffix}_bidirectonal_matrix.url.txt",
           zip, region=DF_SAM_NORM['Region'], index=DF_SAM_NORM['Index'], covarg=DF_SAM_NORM['Value'], 
           suffix=DF_SAM_NORM['Suffix']
        )
        

# BW with deeptools bamCoverage
include: "rules/04_get_BW_Stranded.snake"
# 5 sense matrix file
include: "rules/05_bidirectional_matrix.snake"


