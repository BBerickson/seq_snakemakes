# ====== Rules for filtering and subsampleing aligned reads with samtools =================================

# gets number for norm fraction sample
def _get_norm_samp(wildcards):
    sample = wildcards.sample + "_" + INDEX_SAMPLE
    filename =  PROJ + "/stats/" + PROJ + "_" + INDEX_SAMPLE + "_subsample_frac.tsv"
    num = 0
    with open(filename, "r") as file:
      for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 3 and parts[0] == sample:
          num = parts[2]
          break
    return float(num)
    

# Remove duplicate reads
rule dedup_UMI:
    input:
        bam   = PROJ  + "/{sample}_" + INDEX_MAP + ".bam",
        bai   = PROJ  + "/{sample}_" + INDEX_MAP + ".bam.bai"
    output:
        bam    = temp(PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam"),
        bai    = temp(PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam.bai"),
        counts = PROJ + "/UMI/{sample}_" + INDEX_MAP + "_dedup.txt",
        stats  = PROJ + "/bams/{sample}_" + INDEX_MAP + "_UMI_dedup_stats.txt"
    params:
        job_name = "{sample}_dedup",
        memory   = MEMORY * 3,
        samp     = "{sample}_" + INDEX_MAP,
        args     = CMD_PARAMS["umi_tools"]
    log:
        out = PROJ + "/logs/{sample}_dedup.out",
        err = PROJ + "/logs/{sample}_dedup.err"
    message:
        "Removing duplicates for {wildcards.sample}"
    threads:
        1
    shell:
        """
        umi_tools dedup \
            {params.args} \
            -I {input.bam} \
            -S {output.bam} \
            -L {output.stats}

        samtools index {output.bam}
        echo "{params.samp} Filtered_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" > {output.counts}
        
        """

# filter aligned reads
rule align_filter_sample:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam.bai",
        counts = PROJ + "/UMI/{sample}_" + INDEX_MAP + "_dedup.txt"
    output:
        bam    = PROJ      + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ      + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai",
        counts = PROJ      + "/bams/{sample}_" + INDEX_SAMPLE + "_filtred_count.txt",
        bamt   = temp(PROJ + "/bams/{sample}_temp.bam"),
        bait   = temp(PROJ + "/bams/{sample}_temp2.bam")
    params:
        job_name = "{sample}_align_filter",
        memory   = MEMORY * 3,
        idx      = INDEX_PATH + INDEX_SAMPLE + ".txt",
        samp     = "{sample}_" + INDEX_SAMPLE,
        fa2      = FA_SAMPLE,
        mask     = MASK,
        sortname = PROJ + "/{sample}.temp"
    log:
        out = PROJ + "/logs/{sample}_align_filter.out",
        err = PROJ + "/logs/{sample}_align_filter.err"
    message:
        "filtering reads {wildcards.sample}"
    threads: 
        12
    shell:
        """
        # filter
        samtools view {input.bam} $(head -n1 {params.idx}) \
        | samtools view -bT {params.fa2} - \
        | samtools view -L {params.mask} -b - -U {output.bamt} > {output.bait}
        
        samtools sort {output.bamt} -T {params.sortname} -@ {threads} -O bam \
        > {output.bam}
        
        samtools index -@ {threads} {output.bam}
        echo "{params.samp} Filtered_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" > {output.counts}
        
        """
        
# calculating subsample numbers for group
rule calculating_subsample_group:
    input:
        lambda wildcards: expand(
            PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_filtred_count.txt",
            sample = SAMPLES[wildcards.group]
        )
    output:
        PROJ + "/bams/{group}_" + INDEX_SAMPLE + "_subsample_frac.txt"
    params:
        job_name = "{group}_subsample",
        group   = "{group}",
        memory   = 4
    log:
        out = PROJ + "/logs/{group}_subsample.out",
        err = PROJ + "/logs/{group}_subsample.err"
    message:
        "getting subsample norm for {wildcards.group}"
    threads: 
        1
    script:
        '../R_scripts/count_subsample.R'


# Combine bamCoverage summaries
rule calculating_subsample_summary:
    input:
        sorted(expand(
            PROJ + "/bams/{group}_" + INDEX_SAMPLE + "_subsample_frac.txt",
            group = GRPS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SAMPLE + "_subsample_frac.tsv"
    params:
        job_name = PROJ + "_calculating_subsample_summary",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_calculating_subsample_summary.out",
        err = PROJ + "/logs/" + PROJ + "_calculating_subsample_summary.err"
    message:
        "Creating " + PROJ + " subsample summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)



# output temp subsampled bamfile from dedup UMI
rule subsample_dedup:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai",
        counts = PROJ + "/stats/" + PROJ + "_" + INDEX_SAMPLE + "_subsample_frac.tsv"
    output:
        bam   = PROJ + "/bams_sub/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/bams_sub/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai",
        stats = PROJ + "/bams_sub/{sample}_" + INDEX_SAMPLE + "_subsample.txt"
    params:
        job_name = "subsample_dedup_{sample}",
        memory   = MEMORY * 3,
        samp     = "{sample}_" + INDEX_SAMPLE,
        scale    = _get_norm_samp
    log:
        out = PROJ + "/logs/subsample_dedup_{sample}_" + INDEX_SAMPLE + ".out",
        err = PROJ + "/logs/subsample_dedup_{sample}_" + INDEX_SAMPLE + ".err"
    message:
        "Subsampling UMI_dedup reads for {wildcards.sample} INDEX_SAMPLE "
    threads:
        16
    shell:
        """
          # find the min read count, calcuate the fraction of the reads to keep, samtools subset 
          samtools view -@ {threads} -s {params.scale} -b {input.bam} > {output.bam}
          
          samtools index -@ {threads} {output.bam}
        
          echo "{params.samp} Sampled_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" >> {output.stats}
        """

# Create duplication summary
rule dedup_summary:
    input:
        sorted(expand(
            PROJ + "/bams/{sample}_" + INDEX_MAP + "_UMI_dedup_stats.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_MAP + "_UMI_dedup.tsv"
    params:
        job_name = PROJ + "_dedup_summary",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_dedup_summary.out",
        err = PROJ + "/logs/" + PROJ + "_dedup_summary.err"
    message:
        "Creating " + PROJ + " dedup summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            metrics = [
                "Input Reads: [0-9]+",
                "Number of reads out: [0-9]+",
                "Total number of positions deduplicated: [0-9]+",
                "Mean number of unique UMIs per position: [0-9\.]+",
                "Max. number of unique UMIs per position: [0-9]+"
            ]

            for file in input:
                name  = os.path.basename(file)
                name  = re.sub("_dedup_stats.txt", "", name)

                for line in open(file, "r"):
                    for metric in metrics:
                        met = re.search(metric, line)

                        if met:
                            met = met.group(0)
                            num = re.search("[0-9\.]+$", met).group(0)
                            met = re.sub(": [0-9\.]+$", "", met)

                            out.write("%s\t%s\t%s\n" % (name, met, num))


# Create subsample_whole summary
rule subsample_summary:
    input:
        expand(
            PROJ + "/bams_sub/{sample}_" + INDEX_SAMPLE + "_subsample.txt",
            sample =SAMS_UNIQ)
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SAMPLE + "_subsample.tsv"
    params:
        job_name = PROJ + "_subsample_whole_summary",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_subsample_sample_summary.out",
        err = PROJ + "/logs/" + PROJ + "_subsample_sample_summary.err"
    message:
        "Creating " + PROJ + " subsample_whole summary"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)


# gets number for norm fraction sample
def _get_norm_spike(wildcards):
    sample = wildcards.sample + "_" + INDEX_SPIKE
    filename =  PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv"
    num = 0
    with open(filename, "r") as file:
      for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 3 and parts[0] == sample:
          num = parts[2]
          break
    return float(num)
    

# filter aligned reads
rule align_filter_sample_spikeIN:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_MAP + "_" + SEQ_DATE + "_dedup.bam.bai",
        counts = PROJ + "/UMI/{sample}_" + INDEX_MAP + "_dedup.txt"
    output:
        bam    = PROJ      + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ      + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai",
        counts = PROJ      + "/bams/{sample}_" + INDEX_SPIKE + "_filtred_count.txt"
    params:
        job_name = "{sample}_align_filter_spikeIN",
        memory   = MEMORY * 3,
        idx      = INDEX_PATH + INDEX_SPIKE + ".txt",
        samp     = "{sample}_" + INDEX_SPIKE,
        fa2      = FA_SPIKE,
        sortname = PROJ + "/{sample}_spike.temp"
    log:
        out = PROJ + "/logs/{sample}_align_filter_spikeIN.out",
        err = PROJ + "/logs/{sample}_align_filter_spikeIN.err"
    message:
        "filtering reads {wildcards.sample} _spikeIN"
    threads: 
        12
    shell:
        """
        # filter
        samtools view {input.bam} $(head -n1 {params.idx}) \
        | sed 's/spike_*//g' | samtools view -bT {params.fa2} - \
        | samtools sort - -T {params.sortname} -@ {threads} -O bam \
        > {output.bam}
        
        samtools index -@ {threads} {output.bam}
        
        echo "{params.samp} Filtered_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" > {output.counts}
        
        """
        
# calculating subsample numbers for group
rule calculating_subsample_group_spikeIN:
    input:
        lambda wildcards: expand(
            PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_filtred_count.txt",
            sample = SAMPLES[wildcards.group]
        )
    output:
        PROJ + "/bams/{group}_" + INDEX_SPIKE + "_subsample_frac.txt"
    params:
        job_name = "{group}_subsample_spikeIN",
        group   = "{group}",
        memory   = 4
    log:
        out = PROJ + "/logs/{group}_subsample_spikeIN.out",
        err = PROJ + "/logs/{group}_subsample_spikeIN.err"
    message:
        "getting subsample norm for {wildcards.group} _spikeIN"
    threads: 
        1
    script:
        '../R_scripts/count_subsample.R'


# Combine bamCoverage summaries
rule calculating_subsample_summary_spikeIN:
    input:
        sorted(expand(
            PROJ + "/bams/{group}_" + INDEX_SPIKE + "_subsample_frac.txt",
            group = GRPS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv"
    params:
        job_name = PROJ + "_calculating_subsample_summary_spikeIN",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_calculating_subsample_summary_spikeIN.out",
        err = PROJ + "/logs/" + PROJ + "_calculating_subsample_summary_spikeIN.err"
    message:
        "Creating " + PROJ + " subsample summary_spikeIN"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)



# output temp subsampled bamfile from dedup UMI
rule subsample_dedup_spikeIN:
    input:
        bam    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
        bai    = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai",
        counts = PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample_frac.tsv"
    output:
        bam   = PROJ + "/bams_sub/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/bams_sub/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai",
        stats = PROJ + "/bams_sub/{sample}_" + INDEX_SPIKE + "_subsample.txt"
    params:
        job_name = "subsample_dedup_{sample}_spikeIN",
        memory   = MEMORY * 3,
        samp     = "{sample}_" + INDEX_SPIKE,
        scale    = _get_norm_spike
    log:
        out = PROJ + "/logs/subsample_dedup_{sample}_" + INDEX_SPIKE + ".out",
        err = PROJ + "/logs/subsample_dedup_{sample}_" + INDEX_SPIKE + ".err"
    message:
        "Subsampling UMI_dedup reads for {wildcards.sample} INDEX_SPIKE "
    threads:
        16
    shell:
        """
          # find the min read count, calcuate the fraction of the reads to keep, samtools subset 
          samtools view -@ {threads} -s {params.scale} -b {input.bam} > {output.bam}
          
          samtools index -@ {threads} {output.bam}
        
          echo "{params.samp} Sampled_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" >> {output.stats}
        """


# Create subsample_whole summary
rule subsample_summary_spikeIN:
    input:
        expand(
            PROJ + "/bams_sub/{sample}_" + INDEX_SPIKE + "_subsample.txt",
            sample =SAMS_UNIQ)
    output:
        PROJ + "/stats/" + PROJ + "_" + INDEX_SPIKE + "_subsample.tsv"
    params:
        job_name = PROJ + "_subsample_whole_summary_spikeIN",
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_subsample_sample_summary_spikeIN.out",
        err = PROJ + "/logs/" + PROJ + "_subsample_sample_summary_spikeIN.err"
    message:
        "Creating " + PROJ + " subsample_whole summary_spikeIN"
    threads:
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)



                    