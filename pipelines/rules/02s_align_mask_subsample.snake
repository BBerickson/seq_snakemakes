# ====== Rules for filtering and subsampleing aligned reads with samtools =================================


# filter aligned reads
rule align_mask:
    input:
        bam    = PROJ + "/bams/{sample}_aligned_{index}_" + SEQ_DATE + ".bam",
        bai    = PROJ + "/bams/{sample}_aligned_{index}_" + SEQ_DATE + ".bam.bai"
    output:
        bam    = PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask.bam",
        bai    = PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask.bam.bai",
        counts = PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask_count.txt",
        bamt   = temp(PROJ + "/bams_mask/{sample}_aligned_{index}_temp_mask.bam"),
        bait   = temp(PROJ + "/bams_mask/{sample}_aligned_{index}_temp2_mask.bam")
    params:
        job_name = "{sample}_aligned_{index}_align_filter_mask",
        idx      = lambda wildcards: INDEX_PATH + wildcards.index + ".txt",
        samp     = "{sample}_{index}",
        fa2      = lambda wildcards: config_indexes[wildcards.index].get("FA_SAMPLE"),
        mask     = lambda wildcards: config_indexes[wildcards.index].get("MASK"),
        sortname = PROJ + "/{sample}_aligned_{index}.temp"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.5, 2)
    log:
        out = PROJ + "/logs/{sample}_aligned_{index}_align_filter_mask.out",
        err = PROJ + "/logs/{sample}_aligned_{index}_align_filter_mask.err"
    threads: 
        12
    shell:
        """
        # Conditional masking
        if [ -n "{params.mask}" ] && [ "{params.mask}" != "None" ]; then
            samtools view -L {params.mask} {input.bam} -U {output.bamt} > {output.bait}
        else
            # No mask: copy all reads to bamt, empty file to bait
            cp {input.bam} {output.bamt}
            touch {output.bait}  # Create empty file

        fi
        # sort
        samtools sort {output.bamt} -T {params.sortname} -@ {threads} -O bam > {output.bam}
        
        samtools index -@ {threads} {output.bam}
        echo "{params.samp} Filtered_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" > {output.counts}
        
        """
        
# calculating subsample numbers for group
rule calculating_subsample_group:
    input:
        lambda wildcards: expand(
            PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask_count.txt",
            sample = GROUPS[wildcards.group],
            index = [wildcards.index]
        )
    output:
        temp(PROJ + "/bams_mask/{group}_aligned_{index}_subsample_frac.txt")
    params:
        job_name = "{group}_aligned_{index}_subsample",
        group   = "{group}"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/{group}_aligned_{index}_subsample.out",
        err = PROJ + "/logs/{group}_aligned_{index}_subsample.err"
    threads: 
        1
    script:
        '../R_scripts/count_subsample.R'


# Combine bamCoverage summaries
rule calculating_subsample_summary:
    input:
        lambda wildcards: sorted(expand(
            PROJ + "/bams_mask/{group}_aligned_{index}_subsample_frac.txt",
            group = GRPS_UNIQ,
            index = [wildcards.index]
        ))
    output:
        PROJ + "/stats/" + PROJ + "_{index}_subsample_frac.tsv"
    params:
        job_name = PROJ + "_calculating_{index}_subsample_summary"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/" + PROJ + "_calculating_{index}_subsample_summary.out",
        err = PROJ + "/logs/" + PROJ + "_calculating_{index}_subsample_summary.err"
    threads:
        1
    shell:
        """
        cat {input} > {output}
        """



# output temp subsampled bamfile from dedup UMI
rule subsample_dedup:
    input:
        bam    = PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask.bam",
        bai    = PROJ + "/bams_mask/{sample}_aligned_{index}_" + SEQ_DATE + "_mask.bam.bai",
        counts = PROJ + "/stats/" + PROJ + "_{index}_subsample_frac.tsv"
    output:
        bam   = PROJ + "/bams_sub/{sample}_aligned_{index}_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/bams_sub/{sample}_aligned_{index}_" + SEQ_DATE + ".bam.bai",
        stats = PROJ + "/bams_sub/{sample}_aligned_{index}_subsample.txt"
    params:
        job_name = "subsample_dedup_{sample}_aligned_{index}",
        samp     = "{sample}_{index}",
        scale    = lambda wildcards, input: _get_norm_fraction(wildcards, wildcards.index, input.counts)
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.5, 2)
    log:
        out = PROJ + "/logs/subsample_dedup_{sample}_aligned_{index}.out",
        err = PROJ + "/logs/subsample_dedup_{sample}_aligned_{index}.err"
    threads:
        16
    shell:
        """
          # find the min read count, calcuate the fraction of the reads to keep, samtools subset 
          samtools view -@ {threads} -s {params.scale} -b {input.bam} > {output.bam}
          
          samtools index -@ {threads} {output.bam}
        
          echo "{params.samp} Sampled_reads $(samtools idxstats {output.bam} | awk '{{s+=$3}} END {{print s}}')" >> {output.stats}
        """

# Create subsample_whole summary
rule subsample_summary:
    input:
        lambda wildcards: expand(
            PROJ + "/bams_sub/{sample}_aligned_{index}_subsample.txt",
            sample =SAMS_UNIQ,
            index = [wildcards.index])
    output:
        PROJ + "/stats/" + PROJ + "_{index}_subsample.tsv"
    params:
        job_name = PROJ + "_{index}_subsample_whole_summary"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/" + PROJ + "_{index}_subsample_sample_summary.out",
        err = PROJ + "/logs/" + PROJ + "_{index}_subsample_sample_summary.err"
    threads:
        1
    shell:
        """
        cat {input} > {output}
        """






                    