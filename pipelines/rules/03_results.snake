# ====== Rules for getting overall results =================================

# get read counts from bams
rule featureCounts:
    input:
        bam   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
        PROJ + "/counts/{sample}_" + INDEX_SAMPLE + "_featureCounts.tsv"
    params:
        job_name = "{sample}_bowtie_featureCounts",
        args     = CMD_PARAMS["featureCounts"],
        saf      = SAF
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.5, 4)
    log:
        out = PROJ + "/logs/{sample}_featureCounts.out",
        err = PROJ + "/logs/{sample}_featureCounts.err"
    message:
        "featureCounts for {wildcards.sample}"
    threads:
        12
    shell:
        """
        featureCounts \
            {params.args} \
            -F SAF \
            -a '{params.saf}' \
            -o '{output}' \
            -T {threads} \
            {input.bam}
        
        """
        
# making count file
rule featurecount_summary:
    input:
        PROJ + "/counts/{sample}_" + INDEX_SAMPLE + "_featureCounts.tsv"
    output:
        PROJ  + "/counts/{sample}_count.txt"
    params:
        job_name = "{sample}_count_summary"
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/{sample}_count_summary.out",
        err = PROJ + "/logs/{sample}_count_summary.err"
    threads: 
        1
    script:
        '../R_scripts/featurecount_summary.R'
  

# gathering results
rule gathering_results:
    input:
        PROJ + "/stats/" + PROJ + "_clumpify.tsv",
        PROJ + "/stats/" + PROJ + "_bbduk.tsv",
        PROJ + "/stats/" + PROJ + "_aligned.tsv",
        expand( PROJ + "/counts/{sample}_count.txt",
            sample = SAMS_UNIQ
        )
    output:
        PROJ + "/stats/" + PROJ + "_results.tsv"
    params:
        job_name = "results",
        project  = PROJ
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/results.out",
        err = PROJ + "/logs/results.err"
    message:
        "getting final results"
    threads: 
        1
    script:
        '../R_scripts/results.R'
  

# fragment size of mapped paired end reads
rule fragment_size:
    input:
      bam = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
      bai = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
      log =  PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
      pic =  PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png"
    params:
        job_name = "{sample}_PE_fragment_size",
        labels   = "{sample}", 
        args     = CMD_PARAMS["fragment_size"]
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_PE_fragment_size.out",
        err = PROJ + "/logs/{sample}_PE_fragment_size.err"
    message:
        "fragment size of mapped paired end {wildcards.sample} reads"
    threads: 
        12
    shell:
        """
        bamPEFragmentSize \
          -hist {output.pic} \
          --numberOfProcessors {threads} \
          -T "Fragment size of PE data" \
          -b  {input.bam} {params.args} \
          --samplesLabel {params.labels} \
          --table {output.log}
    
        """

# gathering results fragment size
rule fragment_results:
    input:
        PROJ + "/stats/" + PROJ + "_results.tsv",
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    params:
        job_name = "fragment_results",
        samp     = INDEX_SAMPLE,
        project  = PROJ
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/fragment_results.out",
        err = PROJ + "/logs/fragment_results.err"
    message:
        "getting final fragment_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R'

rule create_plots:
    input:
        PROJ + "/stats/" + PROJ + "_results.tsv",
        expand( PROJ + "/counts/{sample}_count.txt",
            sample = SAMS_UNIQ
        ),
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    output:
        PROJ + "/" + PROJ + "_" + INDEX_SAMPLE + "_qc_analysis.html"
    params:
        job_name   = PROJ + "_qc_plots"
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_qc_plots.out",
        err = PROJ + "/logs/" + PROJ + "_qc_plots.err"
    message:
        "Creating qc plots for " + PROJ + " project"
    threads:
        1
    shell:
        """
        touch .here
        Rmd=src/Rmds/analysis.Rmd
        script=src/Rmds/knit_rmd.R
        samp=samples.yaml
        out="_qc_analysis"

        Rscript $script \
            -i $Rmd \
            -o $out \
            -p {PROJ} \
            -m {INDEX_MAP} \
            -x {INDEX_SAMPLE} \
            -s $samp 
        """
  
      