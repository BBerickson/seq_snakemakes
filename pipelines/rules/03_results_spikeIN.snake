# ====== Rules for getting overall results =================================

# get read counts from bams
rule featureCounts:
    input:
        bam   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
        PROJ + "/counts/{sample}_" + INDEX_SAMPLE + "_featureCounts.tsv"
    params:
        job_name = "{sample}_bowtie_featureCounts",
        args     = CMD_PARAMS["featureCounts"],
        saf      = SAF
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.5, 4)
    log:
        out = PROJ + "/logs/{sample}_featureCounts.out",
        err = PROJ + "/logs/{sample}_featureCounts.err"
    message:
        "featureCounts for {wildcards.sample}"
    threads:
        12
    shell:
        """
        featureCounts \
            {params.args} \
            -F SAF \
            -a '{params.saf}' \
            -o '{output}' \
            -T {threads} \
            {input.bam}
        
        """

# get read counts from bams spikein
rule featureCounts_spikein:
    input:
        bam   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
        bai   = PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai"
    output:
        PROJ + "/counts/{sample}_" + INDEX_SPIKE + "_featureCounts.tsv"
    params:
        job_name = "{sample}_bowtie_featureCounts_spikin",
        args     = CMD_PARAMS["featureCounts"],
        saf      = SAF_SPIKE
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.5, 4)
    log:
        out = PROJ + "/logs/{sample}_featureCounts_spikin.out",
        err = PROJ + "/logs/{sample}_featureCounts_spikins.err"
    message:
        "featureCounts for spikein {wildcards.sample}"
    threads:
        12
    shell:
        """
        featureCounts \
            {params.args} \
            -F SAF \
            -a '{params.saf}' \
            -o '{output}' \
            -T {threads} \
            {input.bam}
        
        """

# making count file
rule featurecount_summary:
    input:
        PROJ + "/counts/{sample}_" + INDEX_SAMPLE + "_featureCounts.tsv",
        PROJ + "/counts/{sample}_" + INDEX_SPIKE + "_featureCounts.tsv"
    output:
        PROJ  + "/counts/{sample}_spikin_count.txt"
    params:
        job_name = "{sample}_count_summary"
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/{sample}_count_summary.out",
        err = PROJ + "/logs/{sample}_count_summary.err"
    threads: 
        1
    script:
        '../R_scripts/featurecount_summary.R'

# calculating OR
rule calculating_OR:
    input:
        lambda wildcards: expand(
            PROJ  + "/counts/{sample}_spikin_count.txt",
            sample = GROUPS[wildcards.group]
        )
    output:
        temp(PROJ + "/counts/{group}_OR_count.txt")
    params:
        job_name = "{group}_OR",
        type     = NORM,
        in_sam   = INDEX_SAMPLE,
        in_spk   = INDEX_SPIKE
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/{group}_OR.out",
        err = PROJ + "/logs/{group}_OR.err"
    message:
        "getting OR norm for {wildcards.group}"
    threads: 
        1
    script:
        '../R_scripts/count_OR.R'
  
# gathering results
rule gathering_OR_results:
    input:
        expand(
        PROJ      + "/counts/{group}_OR_count.txt",
            group = GRPS_UNIQ
        )
    output:
        PROJ + "/counts/" + PROJ + "_OR_summery.tsv"
    params:
        job_name = "ORresults"
    resources:
        memory   = 1
    log:
        out = PROJ + "/logs/ORresults.out",
        err = PROJ + "/logs/ORresults.err"
    message:
        "getting final results"
    threads: 
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)  

# gathering results
rule gathering_results:
    input:
        PROJ + "/stats/" + PROJ + "_clumpify.tsv",
        PROJ + "/stats/" + PROJ + "_bbduk.tsv",
        PROJ + "/stats/" + PROJ + "_aligned.tsv",
        PROJ + "/counts/" + PROJ + "_OR_summery.tsv",
        expand( PROJ + "/counts/{sample}_spikin_count.txt",
            sample = SAMS_UNIQ
        )
    output:
        PROJ + "/stats/" + PROJ + "_results.tsv"
    params:
        job_name = "results",
        project  = PROJ
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/results.out",
        err = PROJ + "/logs/results.err"
    message:
        "getting final results"
    threads: 
        1
    script:
        '../R_scripts/results.R'
  
  
# fragment size of mapped paired end reads
rule fragment_size_samp:
    input:
      bam  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
      bai  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai",
      bam2 = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
      bai2 = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai"
    output:
      log  = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
      log2 = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt",
      pic  = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png",
      pic2 = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragmentSize.png"
    params:
        job_name = "{sample}_PE_fragment_size_sample",
        labels   = "{sample}", 
        args     = CMD_PARAMS["fragment_size"]
    resources:
        memory   = lambda wildcards, input: memory_estimator([input.bam, input.bam2], 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_PE_fragment_sample_size.out",
        err = PROJ + "/logs/{sample}_PE_fragment_sample_size.err"
    message:
        "fragment size of mapped paired end reads for {wildcards.sample}"
    threads: 
        12
    shell:
        """
        bamPEFragmentSize \
          -hist {output.pic2} \
          --numberOfProcessors {threads} \
          -T "Fragment size of PE data" \
          -b  {input.bam2} {params.args} \
          --table {output.log2} 
    
        bamPEFragmentSize \
          -hist {output.pic} \
          --numberOfProcessors {threads} \
          -T "Fragment size of PE data" \
          -b  {input.bam} {params.args} \
          --samplesLabel {params.labels} \
          --table {output.log} 
        
        """

# gathering results fragment size
rule fragment_results:
    input:
        PROJ + "/stats/" + PROJ + "_results.tsv",
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
            sample = SAMS_UNIQ
        )),
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    params:
        job_name = "fragment_results",
        samp     = INDEX_SAMPLE,
        spik     = INDEX_SPIKE,
        project  = PROJ
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/fragment_results.out",
        err = PROJ + "/logs/fragment_results.err"
    message:
        "getting final fragment_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R'
  
rule create_plots:
    input:
        PROJ + "/stats/" + PROJ + "_results.tsv",
        expand( PROJ + "/counts/{sample}_spikin_count.txt",
            sample = SAMS_UNIQ
        ),
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    output:
        PROJ + "/" + PROJ + "_" + INDEX_SAMPLE + "_qc_analysis.html"
    params:
        job_name   = PROJ + "_qc_plots"
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/" + PROJ + "_qc_plots.out",
        err = PROJ + "/logs/" + PROJ + "_qc_plots.err"
    message:
        "Creating qc plots for " + PROJ + " project"
    threads:
        1
    shell:
        """
        touch .here
        Rmd=src/Rmds/analysis.Rmd
        script=src/Rmds/knit_rmd.R
        samp=samples.yaml
        out="_qc_analysis"

        Rscript $script \
            -i $Rmd \
            -o $out \
            -p {PROJ} \
            -m {INDEX_MAP} \
            -x {INDEX_SAMPLE} \
            -s $samp 
        """
 
    