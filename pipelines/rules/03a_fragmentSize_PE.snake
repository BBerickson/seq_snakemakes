# ====== Rules for getting read and fragment size from paired end data =================================


# fragment size of mapped paired end reads
rule fragment_size_samp:
    input:
      bam  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
      bai  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
      log  = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
      pic  = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png"
    params:
        job_name = "{sample}_PE_fragment_size_sample",
        labels   = "{sample}", 
        args     = CMD_PARAMS["fragment_size"]
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_PE_fragment_sample_size.out",
        err = PROJ + "/logs/{sample}_PE_fragment_sample_size.err"
    message:
        "fragment size of mapped paired end reads for {wildcards.sample}"
    threads: 
        12
    shell:
        """
        bamPEFragmentSize \
          -hist {output.pic} \
          --numberOfProcessors {threads} \
          -T "Fragment size of PE data" \
          -b  {input.bam} {params.args} \
          --samplesLabel {params.labels} \
          --table {output.log} 
        
        """
        
# fragment size of mapped paired end reads
rule fragment_size_spike:
    input:
      bam = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
      bai = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai"
    output:
      log = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt",
      pic = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragmentSize.png"
    params:
        job_name = "{sample}_PE_fragment_size_spike",
        labels   = "{sample}_spike", 
        args     = CMD_PARAMS["fragment_size"]
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_PE_fragment_spike_size.out",
        err = PROJ + "/logs/{sample}_PE_fragment_spike_size.err"
    message:
        "fragment size of mapped paired end reads for {wildcards.sample} spikein"
    threads: 
        12
    shell:
        """
        bamPEFragmentSize \
          -hist {output.pic} \
          --numberOfProcessors {threads} \
          -T "Fragment size of PE data" \
          -b  {input.bam} {params.args} \
          --samplesLabel {params.labels} \
          --table {output.log} 
        
        """


# gathering results fragment size
rule fragment_results:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    params:
        job_name = "fragment_results",
        samp     = INDEX_SAMPLE
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_results.out",
        err = PROJ + "/logs/fragment_results.err"
    message:
        "getting final fragment_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R'
  

# gathering results fragment size
rule fragment_results_spike:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_spike_fragment_results.tsv"
    params:
        job_name = "fragment_spike_results",
        samp     = INDEX_SPIKE
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_spike_results.out",
        err = PROJ + "/logs/fragment_spike_results.err"
    message:
        "getting final fragment_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R' 
  
# gathering png fragment size
rule fragment_png:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/report/" + PROJ + "_fragmentSize.pdf"
    params:
        job_name  = "fragment_results_pdf",
        chunkSize = 4
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_results_pdf.out",
        err = PROJ + "/logs/fragment_results_pdf.err"
    threads: 
        1
    script:
        '../R_scripts/pngToPDF.R'
  

# gathering png fragment size
rule fragment_pdf_spike:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragmentSize.png",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/report/" + PROJ + "_spike_fragmentSize.pdf"
    params:
        job_name  = "fragment_spike_results_pdf",
        chunkSize = 4
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_spike_results_pdf.out",
        err = PROJ + "/logs/fragment_spike_results_pdf.err"
    threads: 
        1
    script:
        '../R_scripts/pngToPDF.R' 
    