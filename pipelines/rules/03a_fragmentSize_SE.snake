# ====== Rules for getting read and read size from single end data =================================


# read size of mapped paired end reads
rule read_size_samp:
    input:
      bam  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam",
      bai  = PROJ + "/bams/{sample}_" + INDEX_SAMPLE + "_" + SEQ_DATE + ".bam.bai"
    output:
      hist = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt"
    params:
        job_name = "{sample}_SE_read_size_sample"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_SE_read_sample_size.out",
        err = PROJ + "/logs/{sample}_SE_read_sample_size.err"
    message:
        "read size of mapped single end reads for {wildcards.sample}"
    threads: 
        1
    shell:
        """
        # bbtools
        readlength.sh \
          in={input.bam} \
          out={output.hist} \
          bin=10 max=1010
        """
        
# png of read size of mapped single end reads
rule read_size_png:
    input:
      hist = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt"
    output:
      pic = PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png"
    params:
        job_name = "{sample}_SE_fragment_png"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/{sample}_SE_read_size_png.out",
        err = PROJ + "/logs/{sample}_SE_read_size.err"
    message:
        "read size of mapped paired end reads for {wildcards.sample} png"
    threads: 
        1
    script:
        '../R_scripts/read_size_png.R'

# read size of mapped paired end reads
rule read_size_samp_spike:
    input:
      bam  = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
      bai  = PROJ + "/bams/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam.bai"
    output:
      hist = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt"
    params:
        job_name = "{sample}_SE_read_size_spike"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 0.2, 5)
    log:
        out = PROJ + "/logs/{sample}_SE_read_spike_size.out",
        err = PROJ + "/logs/{sample}_SE_read_spike_size.err"
    threads: 
        1
    shell:
        """
        # bbtools
        readlength.sh \
          in={input.bam} \
          out={output.hist} \
          bin=10 max=1010
        """
        
# png of read size of mapped single end reads
rule read_size_png_spike:
    input:
      hist = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt"
    output:
      pic = PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragmentSize.png"
    params:
        job_name = "{sample}_SE_fragment_png_spike"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/{sample}_spike_SE_read_size_png.out",
        err = PROJ + "/logs/{sample}_spike_SE_read_size.err"
    threads: 
        1
    script:
        '../R_scripts/read_size_png.R'
        
# gathering results read size
rule read_results:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_fragment_results.tsv"
    params:
        job_name = "read_results",
        samp     = INDEX_SAMPLE
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/read_results.out",
        err = PROJ + "/logs/read_results.err"
    message:
        "getting final read_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R'
  

# gathering results read size
rule read_results_spike:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragment.txt",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/stats/" + PROJ + "_spike_fragment_results.tsv"
    params:
        job_name = "read_spike_results",
        samp     = INDEX_SPIKE
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/read_spike_results.out",
        err = PROJ + "/logs/read_spike_results.err"
    message:
        "getting final read_results"
    threads: 
        1
    script:
        '../R_scripts/fragment_results.R' 
  
# gathering png fragment size
rule fragment_png:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SAMPLE + "_fragmentSize.png",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/report/" + PROJ + "_fragmentSize.pdf"
    params:
        job_name  = "fragment_results_pdf",
        chunkSize = 4
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_results_pdf.out",
        err = PROJ + "/logs/fragment_results_pdf.err"
    threads: 
        1
    script:
        '../R_scripts/pngToPDF.R'
  

# gathering png fragment size
rule fragment_pdf_spike:
    input:
        sorted(expand(
            PROJ + "/stats/{sample}_" + INDEX_SPIKE + "_fragmentSize.png",
            sample = SAMS_UNIQ
        ))
    output:
        PROJ + "/report/" + PROJ + "_spike_fragmentSize.pdf"
    params:
        job_name  = "fragment_spike_results_pdf",
        chunkSize = 4
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/fragment_spike_results_pdf.out",
        err = PROJ + "/logs/fragment_spike_results_pdf.err"
    threads: 
        1
    script:
        '../R_scripts/pngToPDF.R' 
    
    