# ====== Rules for getting overall results =================================

# multiBamSummary spikein reads
rule multiBamSummary:
    input:
        bam = lambda wildcards: expand(
            PROJ + "/" + BAM_PATH + "/{sample}_aligned_{index}_" + SEQ_DATE + ".bam",
            sample = GROUPS[wildcards.group],
            index = [wildcards.index]
        ),
        counts = lambda wildcards: expand(
          PROJ + "/stats/{sample}_summary_featureCounts.tsv",
            sample = GROUPS[wildcards.group]
        )
    output:
        npz  = PROJ + "/" + BAM_PATH + "/{group}_aligned_{index}_" + SEQ_DATE + ".npz",
        gsf  = PROJ + "/" + BAM_PATH + "/{group}_aligned_{index}_" + SEQ_DATE + "_scalefactor.txt"
    params:
        job_name = "{group}_aligned_{index}_multiBamSummary",
        index_sample = INDEXES,
        labs = lambda wildcards: " ".join(GROUPS[wildcards.group]),
        script = "pipelines/R_scripts/multiBamSummary.R"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input.bam, 0.2, 5)
    log:
        out = PROJ + "/logs/{group}_aligned_{index}_multiBamSummary.out",
        err = PROJ + "/logs/{group}_aligned_{index}_multiBamSummary.err"
    threads: 
        12
    shell:
        """
          multiBamSummary bins \
            --binSize 100 \
            --bamfiles {input.bam} \
            --labels {params.labs} \
            -p {threads} \
            --scalingFactors {output.gsf} \
            -o {output.npz}
        
          Rscript {params.script} {output.gsf} "{input.counts}" "{params.index_sample}"
        
        """

# calculating OR
rule calculating_OR:
    input:
        lambda wildcards: expand(
            PROJ  + "/stats/{sample}_summary_featureCounts.tsv",
            sample = SAMPIN[wildcards.newnam]
        )
    output:
        temp(PROJ + "/counts/{newnam}_OR_count.txt")
    params:
        job_name = "{newnam}_OR",
        type     = NORM,
        indexs   = INDEXES,
        script   = "pipelines/R_scripts/count_OR.R"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/{newnam}_OR.out",
        err = PROJ + "/logs/{newnam}_OR.err"
    message:
        "getting OR norm for {wildcards.newnam}"
    threads: 
        1
    shell:
        """
        Rscript {params.script} "{input}" "{params.type}" "{params.indexs}" "{output}"

        """
  
# gathering results
rule gathering_OR_results:
    input:
        expand(
        PROJ      + "/counts/{newnam}_OR_count.txt",
            newnam = NAMS_UNIQ
        )
    output:
        PROJ + "/counts/" + PROJ + "_OR_summery.tsv"
    params:
        job_name = "ORresults"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/ORresults.out",
        err = PROJ + "/logs/ORresults.err"
    message:
        "getting final results"
    threads: 
        1
    shell:
        """
        cat {input} > {output}
        """
                    
# gathering results
rule gathering_results:
    input:
        clump   = PROJ + "/stats/" + PROJ + "_clumpify.tsv",
        bbduk   = PROJ + "/stats/" + PROJ + "_bbduk.tsv",
        aligned = PROJ + "/stats/" + PROJ + "_aligned.tsv",
        fc_file = expand( PROJ + "/stats/{sample}_summary_featureCounts.tsv",
            sample = SAMS_UNIQ
        ),
        OR      = PROJ + "/counts/" + PROJ + "_OR_summery.tsv",
        rf_file = expand( PROJ + "/counts/{sample}_aligned_{index}_norm_{suffix}_refCounts.tsv",
            sample = SAMS_UNIQ,
            index  = INDEXES,
            suffix=DF_SAM_NORM['Suffix'].unique() 
        )
    output:
        PROJ + "/report/" + PROJ + "_results.tsv"
    params:
        job_name = "results",
        indexmap = INDEX_MAP,
        sam_new  = SAMPLES,
        project  = PROJ,
        filterty = CMD_PARAMS["filter_results"],
        script = "pipelines/R_scripts/results.R"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/results.out",
        err = PROJ + "/logs/results.err"
    message:
        "getting final results"
    threads: 
        1
    shell:
        """
        Rscript {params.script} "{input.clump}" "{input.bbduk}" "{input.aligned}" "{input.fc_file}" "{params.indexmap}" "{params.sam_new}" "{params.project}" "{params.filterty}" "{output}"

        """
  
rule create_plots:
    input:
        PROJ + "/report/" + PROJ + "_results.tsv"
    output:
        SEQ_DATE + "_" + PROJ + "_" + INDEXES[0] + "_qc_analysis.html"
    params:
        job_name   = PROJ + "_qc_html"
    resources:
        memory   = lambda wildcards, input: memory_estimator(input, 1, 1) # multiplier, min, max, unit=GB
    log:
        out = PROJ + "/logs/" + PROJ + "_qc_html.out",
        err = PROJ + "/logs/" + PROJ + "_qc_html.err"
    threads:
        1
    shell:
        """
        touch .here
        Rmd=pipelines/src/Rmds/analysis.Rmd
        script=pipelines/src/Rmds/knit_rmd.R
        out="_qc_analysis"

        Rscript $script \
            -i $Rmd \
            -o $out \
            -p {PROJ} \
            -m {INDEX_MAP} \
            -d {SEQ_DATE} \
            -x {INDEXES[0]} \
            -s {SAMPLES_FILE} 
        """
     