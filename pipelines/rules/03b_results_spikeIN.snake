# ====== Rules for getting overall results =================================

# multiBamSummary spikein reads
rule multiBamSummary:
    input:
        bam = lambda wildcards: expand(
            PROJ + "/" + BAM_PATH + "/{sample}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".bam",
            sample = GROUPS[wildcards.group]
        ),
        counts = lambda wildcards: expand(
          PROJ  + "/counts/{sample}_spikin_count.txt",
            sample = GROUPS[wildcards.group]
        )
    output:
        npz  = PROJ + "/" + BAM_PATH + "/{group}_" + INDEX_SPIKE + "_" + SEQ_DATE + ".npz",
        gsf  = PROJ + "/" + BAM_PATH + "/{group}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_scalefactor.txt"
    params:
        job_name = "{group}_multiBamSummary",
        labs = lambda wildcards: " ".join(GROUPS[wildcards.group])
    resources:
        memory   = lambda wildcards, input: memory_estimator(input.bam, 0.2, 5)
    log:
        out = PROJ + "/logs/{group}_multiBamSummary.out",
        err = PROJ + "/logs/{group}_multiBamSummary.err"
    threads: 
        12
    shell:
        """
          multiBamSummary bins \
            --binSize 100 \
            --bamfiles {input.bam} \
            --labels {params.labs} \
            -p {threads} \
            --scalingFactors {output.gsf} \
            -o {output.npz}
        
          Rscript pipelines/R_scripts/multiBamSummary.R {output.gsf} "{input.counts}" {INDEX_SAMPLE}
        
        """
        
# making count file with spikein
rule featurecount_spikein_summary:
    input:
        PROJ + "/counts/{sample}_" + INDEX_SAMPLE + "_featureCounts.tsv",
        PROJ + "/counts/{sample}_" + INDEX_SPIKE + "_featureCounts.tsv"
    output:
        PROJ  + "/counts/{sample}_spikin_count.txt"
    params:
        job_name = "{sample}_spikein_count_summary",
        index = [INDEX_SAMPLE, INDEX_SPIKE]
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/{sample}_spikein_count_summary.out",
        err = PROJ + "/logs/{sample}_spikein_count_summary.err"
    threads: 
        1
    script:
        '../R_scripts/featurecount_summary.R'
        
# calculating OR
rule calculating_OR:
    input:
        lambda wildcards: expand(
            PROJ  + "/counts/{sample}_spikin_count.txt",
            sample = SAMPIN[wildcards.newnam]
        )
    output:
        temp(PROJ + "/counts/{newnam}_OR_count.txt")
    params:
        job_name = "{newnam}_OR",
        type     = NORM,
        in_sam   = INDEX_SAMPLE,
        in_spk   = INDEX_SPIKE
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/{newnam}_OR.out",
        err = PROJ + "/logs/{newnam}_OR.err"
    message:
        "getting OR norm for {wildcards.newnam}"
    threads: 
        1
    script:
        '../R_scripts/count_OR.R'
  
# gathering results
rule gathering_OR_results:
    input:
        expand(
        PROJ      + "/counts/{newnam}_OR_count.txt",
            newnam = NAMS_UNIQ
        )
    output:
        PROJ + "/counts/" + PROJ + "_OR_summery.tsv"
    params:
        job_name = "ORresults"
    resources:
        memory   = 1
    log:
        out = PROJ + "/logs/ORresults.out",
        err = PROJ + "/logs/ORresults.err"
    message:
        "getting final results"
    threads: 
        1
    run:
        with open(output[0], "w") as out:
            for file in input:
                for line in open(file, "r"):
                    out.write(line)
                    
                    
                    
# gathering results
rule gathering_results:
    input:
        PROJ + "/stats/" + PROJ + "_clumpify.tsv",
        PROJ + "/stats/" + PROJ + "_bbduk.tsv",
        PROJ + "/stats/" + PROJ + "_aligned.tsv",
        PROJ + "/counts/" + PROJ + "_OR_summery.tsv",
        PROJ + "/report/" + PROJ + "_fragmentSize.pdf",
        PROJ + "/report/" + PROJ + "_spike_fragmentSize.pdf",
        expand(
        PROJ + "/" + BAM_PATH + "/{group}_" + INDEX_SPIKE + "_" + SEQ_DATE + "_scalefactor.txt",
        group = GRPS_UNIQ
        )
    output:
        PROJ + "/report/" + PROJ + "_results.tsv"
    params:
        job_name = "results",
        project  = PROJ
    resources:
        memory   = 4
    log:
        out = PROJ + "/logs/results.out",
        err = PROJ + "/logs/results.err"
    message:
        "getting final results"
    threads: 
        1
    script:
        '../R_scripts/results.R'
  
  