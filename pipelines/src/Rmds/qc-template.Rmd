
```{r "qc setup"}

# Summary files
stats_dir <- here::here(res_dir, "stats/")
report_dir <- here::here(res_dir, "report/")

fq_sum     <- str_c(stats_dir, proj, "_fastqc.tsv")
trim_sum   <- str_c(stats_dir, proj, "_clumpify.tsv")
trim_cut   <- str_c(stats_dir, proj, "_cutadapt.tsv")
bbduk_sum  <- str_c(stats_dir, proj, "_bbduk.tsv")
dedup_sum  <- str_c(stats_dir, proj, "_", index_map, "_UMI_dedup.tsv")
bwt_sum    <- str_c(stats_dir, proj, "_aligned.tsv")

# Sample names and groups
qc_sams <- lapply(plot_grpss$SAMPLES[[1]], function(x) unlist(x)) %>% bind_cols() 


# Create table of sample groups
grp_key <- qc_sams %>%
  imap_dfr(~ crossing(grp = .y, sample = .x))

# QC theme
qc_theme <- theme(
  panel.border = element_blank(),
  axis.line    = element_line(color = "black", linewidth = ln_pt)
)

```

## FastQC

`FastQC` was used to assess the quality of each fastq file. A summary of the results is shown below.

```{r "fq summary", fig.width = 24, fig.height = 10}

# Some sample names are missing part of the fastq file name
# need to match samples with fastq files
fq_sum <- fq_sum %>%
  read_tsv(col_names = c("value", "metric", "sample"),show_col_types = FALSE) 

fq_sum <- fq_sum %>% 
  mutate(sample = sapply(sample, function(x) grp_key$sample[str_starts(x,grp_key$sample)][1])) %>%
  full_join(grp_key, by = c("sample"))

fq_sum <- fq_sum %>%
  mutate(
    value = fct_relevel(value, distinct(fq_sum,value)$value),
    grp   = fct_relevel(grp, unique(grp_key$grp))
  )

# Create tiles
ratio <- max(c(1,floor(n_distinct(fq_sum$sample)/10)))
fq_sum %>%
  ggplot(aes(sample, metric, fill = value)) +
  geom_tile(color = "white", size = 0.5) +
  facet_wrap(~ grp, scales = "free_x", nrow = ratio) +
  scale_fill_manual(values = theme_colors[c(3, 4, 2)]) +
  theme_info +
  theme(
    aspect.ratio    = ratio,
    legend.position = "top",
    legend.title = element_blank(),
    axis.title   = element_blank(),
    axis.text.x  = element_text(face ="bold",  size = 15, angle = 45, hjust = 1, vjust = 1)
  ) +
  qc_theme




```

<br>

```{r "bbtools summary", fig.width = 16, fig.height = 10, results='asis', echo=FALSE}
mets <- c("Reads_passing_filters","Duplicates_Removed","Illumina_Seq_Removed")

if(file.exists(trim_sum)){
  cat("## bbtools clumpify and bbduk\n\n")
  cat("`Clumpify` was used to remove duplicate sequences. `Bbduk` was used to remove Illumina sequences\n\n")
  
  trim_sum <- read_tsv(trim_sum, col_names = c("sample", "metric", "value"),show_col_types = FALSE)

  bbduk_sum <- read_tsv(bbduk_sum, col_names = c("sample", "metric", "value"),show_col_types = FALSE) %>% 
    separate(.,value,c("value","temp"),"\\(",extra="drop",fill="right",convert=T) %>% 
    dplyr::select(-temp) 
  
  tb_sum <- bind_rows(trim_sum,bbduk_sum)
  tb_sum <- tb_sum  %>% 
    pivot_wider(names_from = "metric", values_from = "value") %>% 
    dplyr::mutate(Reads_passing_filters = `Reads_In` - (`Duplicates_Found` + `Total_Removed`)) %>%
    dplyr::rename(Illumina_Seq_Removed = Total_Removed, Duplicates_Removed = Duplicates_Found ) %>% 
    dplyr::select(-`Reads_In`) %>%
    pivot_longer(cols = -sample, names_to = "metric")
  
  result <- ifelse(n_distinct(grp_key$grp) <= 4, 1, 2)
  # Create bar graphs
  tb_sum %>%
    create_qc_bars(
      grp_df     = grp_key,
      grp_lvls   = names(qc_sams),
      sam_lvls   = NULL,
      met_lvls   = mets,
      plot_cols  = theme_colors,
      lab_metric = "Reads_passing_filters",
      n_rows     = result,
      lab_size   = 20
    ) +
    qc_theme
}
```


<br>

```{r "cutadapt summary", fig.width = 16, fig.height = 10, results='asis', echo=FALSE}
mets <- c("Total_read_pairs_processed","Pairs_written_(passing_filters)","Pairs_that_were_too_short")

if(file.exists(trim_cut)){
  cat("## Cutadapt\n\n")
  cat("`Cutadapt` was used to trim adapters. Reads that were untrimmed or were too short were removed, metrics are shown below. The number of reads written is displayed on each bar.\n\n")
  
  trim_cut <- trim_cut %>%
    read_tsv(col_names = c("sample", "metric", "value"),show_col_types = FALSE) %>%
    dplyr::filter(metric %in% mets) %>%
    separate(.,value,c("value","temp"),"\\(",extra="drop",fill="right",convert=T) %>% 
    dplyr::select(-temp) %>% 
    pivot_wider(names_from = "metric", values_from = "value") %>% 
    dplyr::mutate(Untrimmed = `Total_read_pairs_processed` - (`Pairs_that_were_too_short` + `Pairs_written_(passing_filters)`)) %>%
    dplyr::select(-`Total_read_pairs_processed`) %>%
    pivot_longer(cols = -sample, names_to = "metric")
  
  # Create bar graphs
  trim_cut %>%
    create_qc_bars(
      grp_df     = grp_key,
      grp_lvls   = names(qc_sams),
      sam_lvls   = NULL,
      met_lvls   = rev(c(mets[-1], "Untrimmed")),
      plot_cols  = theme_colors,
      lab_metric = "Pairs_written_(passing_filters)",
      n_rows     = 2
    ) +
    qc_theme
}

```


<br>

## Read alignment

`Bowtie2` was used to align reads, metrics are shown below. The number of aligned reads (aligned exactly 1 time + aligned >1 times) is displayed on each bar.

```{r "bwt summary", fig.width = 16, fig.height = 10}

# Bowtie2 stats
mets <- c(
  "aligned exactly 1 time",
  "aligned >1 times",
  "aligned 0 times"
)

bwt_sum <- bwt_sum %>%
  read_tsv(col_names = c("sample", "metric", "value"),show_col_types = FALSE) %>%
  dplyr::mutate(metric=str_remove(metric,"concordantly ")) %>% 
  dplyr::filter(metric %in% mets) %>%
  dplyr::mutate(value = as.numeric(value))


result <- ifelse(n_distinct(grp_key$grp) <= 4, 1, 2)
# Create bar graphs
bwt_sum %>%
  create_qc_bars(
    grp_df     = grp_key,
    grp_lvls   = names(qc_sams),
    sam_lvls   = NULL,
    met_lvls   = rev(mets),
    plot_cols  = theme_colors[c(2, 5, 4)],
    lab_metric = c("aligned exactly 1 time", "aligned >1 times"),
    n_rows     = result,
    lab_size   = 20
  ) +
  qc_theme

```

<br>


```{r "dedup summary", fig.width = 16, fig.height = 10, results='asis', echo=FALSE}
mets <- c("Input Reads", "Number of reads out")

if(file.exists(dedup_sum)){
  cat("## PCR duplicates\n\n")
  cat("Duplicate reads were removed using `umi-tools`, metrics are shown below. The number of reads written is displayed on each bar.\n\n")

  dup_sum <- dedup_sum %>%
    read_tsv(col_names = c("sample", "metric", "value"),show_col_types = FALSE) %>%
    dplyr::filter(metric %in% mets) %>%
    pivot_wider(names_from = "metric", values_from = "value") %>%
    dplyr::mutate(
      `Duplicated reads` = `Input Reads` - `Number of reads out`,
      sample=str_remove(sample, paste0("_",index_map,"_UMI"))
    ) %>%
    dplyr::select(-`Input Reads`) %>%
    pivot_longer(cols = -sample, names_to = "metric")
  
  # Create bar graphs
  dup_sum %>%
    create_qc_bars(
      grp_df     = grp_key,
      grp_lvls   = names(qc_sams),
      sam_lvls   = NULL,
      met_lvls   = rev(c(mets[-1], "Duplicated reads")),
      plot_cols  = c(theme_colors[1], "#C4C4C4"),
      lab_metric = "Number of reads out",
      n_rows     = 2
    ) +
    qc_theme
}
```

## fragment size plots

```{r "fragment size plots", fig.width = 20, fig.height = 20}

chunkSize <- 4
pdfFile <- list.files(path = report_dir, pattern = "*_fragmentSize.pdf$")
if(length(pdfFile) > 0) {
  pdfFile <- str_c(report_dir, pdfFile)
  
  # Create a list of embed tags
  embed_tags <- lapply(pdfFile, function(i) {
    htmltools::tags$embed(src = i, 
                         type = "application/pdf", 
                         width = "100%", 
                         height = "800px")
  })
  
  # Print the tagList
  htmltools::tagList(embed_tags)
}

```


```{r "PCA plot", fig.width = 16, fig.height = 10, results='asis'}
chunkSize <- 2
pngFiles <- list.files(path = report_dir, pattern = "*_PCA.png$")
if(length(pngFiles)>0){
  cat("## PCA plot\n\n")
  pngFiles <- str_c(report_dir, pngFiles)
  nsize <- length(pngFiles)
  rl = lapply(sprintf(pngFiles), png::readPNG)
  gl = lapply(rl, grid::rasterGrob)
  gridExtra::grid.arrange(grobs=gl,ncol=chunkSize)
}
```

## Session info

```{r, echo = FALSE}

sessionInfo()

```


---

<br>

<br>
